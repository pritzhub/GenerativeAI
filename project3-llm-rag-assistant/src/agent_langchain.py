# pip install langchain langchain-openai

"""
LangChain-based agent that uses the existing RAG pipeline as a tool.

This file assumes:
- src/rag_query.py defines: load_index, retrieve_top_k, build_context, answer_query
- src/settings.py defines: get_setting (for rag.top_k etc.)
"""

from typing import Optional

from langchain_core.tools import tool
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI  # or your provider wrapper

from src.rag_query import load_index, retrieve_top_k, build_context, answer_query
from src.settings import get_setting


# --- RAG Tool ---------------------------------------------------------------

@tool("rag_tool")
def rag_tool(query: str, top_k: Optional[int] = None) -> str:
    """
    Answer a question using the indexed documents.

    Args:
        query: User question about the indexed content.
        top_k: Optional override for number of chunks to retrieve.

    Returns:
        Answer string generated by the LLM using retrieved context.
    """
    k = top_k or int(get_setting("rag.top_k", 5))

    # Load index once per call (fine for now; you can cache later)
    df_index, emb_array = load_index()

    chunks = retrieve_top_k(df_index, emb_array, query, k=k)
    context = build_context(chunks)
    answer = answer_query(query, context)
    return answer


# --- Agent setup ------------------------------------------------------------

def build_agent():
    """
    Build a simple LangChain-style agent that can call rag_tool.
    """

    # Base chat model (swap to your provider if needed)
    llm = ChatOpenAI(
        model="gpt-4.1-mini",
        temperature=0.1,
    )

    tools = [rag_tool]

    # Bind tools to the model (newer LangChain style)
    llm_with_tools = llm.bind_tools(tools)

    # Simple prompt: encourage using rag_tool for doc-related questions
    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                (
                     "You are an assistant with access to 'rag_tool', which answers questions "
                     "using the documents that have been ingested and indexed on disk. "
                     "Whenever the user asks about 'uploaded documents', 'docs', 'my files', "
                     "or any content that might be in those documents, you MUST call 'rag_tool'. "
                     "Only answer directly when the question is clearly general knowledge."
                ),
            ),
            ("human", "{input}"),
        ]
    )

    # Chain = prompt â†’ model-with-tools
    chain = prompt | llm_with_tools
    return chain


def run_agent(question: str) -> str:
    """
    Convenience function to run a single-turn interaction with the agent.

    Args:
        question: User input string.

    Returns:
        Final text answer from the model (tool calls handled implicitly).
    """
    chain = build_agent()

    # Invoke the chain. Depending on LangChain version, `invoke` returns
    # a ChatMessage or dict-like response.
    result = chain.invoke({"input": question})

    # Newer LangChain models often return a `BaseMessage`
    # with `.content` as the text.
    if hasattr(result, "content"):
        return result.content
    elif isinstance(result, dict) and "output_text" in result:
        return result["output_text"]
    else:
        return str(result)

def run_agent(question: str) -> str:
    # For now, always route through RAG, just like rag_query
    return rag_tool(question)


if __name__ == "__main__":
    # Quick manual test from command line
    while True:
        q = input("Question (or 'exit'): ").strip()
        if q.lower() in {"exit", "quit"}:
            break
        answer = run_agent_direct_tools_call(q)
        print("ANSWER:", answer)
        print("-" * 80)
